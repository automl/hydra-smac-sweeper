{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# MLP with Multi-Fidelity\n\nExample for optimizing a Multi-Layer Perceptron (MLP) using multiple budgets.\nSince we want to take advantage of Multi-Fidelity, the SMAC4MF facade is a good choice. By default,\nSMAC4MF internally runs with [hyperband](https://arxiv.org/abs/1603.06560), which is a combination of an\naggressive racing mechanism and successive halving.\n\nMLP is a deep neural network, and therefore, we choose epochs as fidelity type. The digits dataset\nis chosen to optimize the average accuracy on 5-fold cross validation.\n\nThis example is adapted from [](https://github.com/automl/SMAC3/blob/main/examples/2_multi_fidelity/1_mlp_epochs.py).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "__copyright__ = \"Copyright 2022, AutoML.org Freiburg-Hannover\"\n__license__ = \"3-clause BSD\"\n\n\nimport warnings\n\nimport hydra\nimport numpy as np\nfrom omegaconf import DictConfig\nfrom sklearn.datasets import load_digits\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\ndigits = load_digits()\n\n\n# Target Algorithm\n@hydra.main(config_path=\"configs\", config_name=\"mlp\", version_base=\"1.1\")\ndef mlp_from_cfg(cfg: DictConfig):\n    \"\"\"\n    Creates a MLP classifier from sklearn and fits the given data on it.\n\n    Parameters\n    ----------\n    cfg: Configuration\n        configuration chosen by smac\n\n    Returns\n    -------\n    float\n    \"\"\"\n    # For deactivated parameters, the configuration stores None-values.\n    # This is not accepted by the MLP, so we replace them with placeholder values.\n    lr = cfg.learning_rate or \"constant\"\n    lr_init = cfg.learning_rate_init or 0.001\n    batch_size = cfg.batch_size or 200\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n        mlp = MLPClassifier(\n            hidden_layer_sizes=[cfg.n_neurons] * cfg.n_layer,\n            solver=cfg.solver,\n            batch_size=batch_size,\n            activation=cfg.activation,\n            learning_rate=lr,\n            learning_rate_init=lr_init,\n            max_iter=int(np.ceil(cfg.epochs)),\n            random_state=cfg.seed,\n        )\n\n        # returns the cross validation accuracy\n        cv = StratifiedKFold(n_splits=5, random_state=cfg.seed, shuffle=True)  # to make CV splits consistent\n        score = cross_val_score(mlp, digits.data, digits.target, cv=cv, error_score=\"raise\")\n\n    return 1 - np.mean(score)\n\n\nif __name__ == \"__main__\":\n    mlp_from_cfg()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}